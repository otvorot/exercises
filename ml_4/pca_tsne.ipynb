{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ В предыдущих юнитах мы работали преимущественно в двухмерном или трёхмерном пространстве. Наши объекты описывались двумя или тремя признаками. Однако зачастую в реальных задачах объекты описываются куда большим числом признаков, а значит, работать приходится в многомерном пространстве, которое невозможно визуализировать.\n",
    "\n",
    "Трудности могут возникнуть, если:\n",
    "\n",
    "нужно визуализировать результаты кластеризации, а пространство признаков многомерное.\n",
    "нужно обучить модель, а объект описывается большим количеством признаков. Эти признаки могут быть избыточными или малоинформативными, что приведёт к плохим результатам при обучении модели.\n",
    "Избежать этих проблем поможет снижение размерности данных.\n",
    "\n",
    "→ Мы встречались с уменьшением размерности ранее, например когда рассматривали спектральную кластеризацию. В данном юните мы поближе познакомимся с тем, как работают алгоритмы снижения размерности, рассмотрим такие техники, как PCA и t-SNE.\n",
    "\n",
    "Задача снижения размерности — это задача преобразования данных с целью уменьшения количества признаков, которые описывают объект.\n",
    "\n",
    "Как вы можете догадаться, при уменьшении количества признаков мы теряем часть информации. Например, на изображении ниже Губка Боб после уменьшения размерности стал описываться меньшим количеством признаков и поэтому стал выглядеть по-другому.\n",
    "\n",
    "img\n",
    "Источник изображения\n",
    "Методы снижения размерности могут преобразовывать данные двумя способами:\n",
    "\n",
    "линейно,\n",
    "нелинейно.\n",
    "PCA\n",
    "\n",
    "Метод главных компонент, или PCA (Principal Components Analysis) — это один из базовых способов уменьшения размерности.\n",
    "\n",
    "Данный метод имеет широкое применение:\n",
    "\n",
    "Подавление шума на изображениях.\n",
    "\n",
    "Изображение состоит из пикселей, которые можно рассматривать как набор точек в многомерном пространстве. С помощью метода снижения размерности PCA можно преобразовать этот набор точек и оставить только первые компоненты, полученные после преобразования. В этих компонентах будет содержаться основная информация об изображении, но не будет шума. Таким образом мы улучшим качество изображения.\n",
    "\n",
    "Качество картинки с шумом и без него:\n",
    "\n",
    "img\n",
    "Источник изображения\n",
    "Индексация видео для быстрого поиска по базе.\n",
    "\n",
    "Каждый кадр видео можно преобразовать с помощью PCA и представить несколькими значениями. Далее эти значения легко хранить и искать в базе.\n",
    "\n",
    "Для уменьшения размерности метод главных компонент проводит линейное преобразование пространства, которое сохраняет длины векторов. Таким образом происходит отображение признаков в новое пространство с меньшей размерностью.\n",
    "\n",
    "В новом пространстве появляются новые оси. Они строятся таким образом, что для первой оси дисперсия данных должна быть максимальной, а вторая ось ортогональна первой и имеет максимально возможную дисперсию.\n",
    "\n",
    "Первой главной компонентой будет называться первая ось в новом пространстве.\n",
    "\n",
    "Например, если у нас есть данные о полученных студентами баллах на экзаменах по двум предметам, мы можем визуализировать эти данные в 2D-пространстве, где по оси x будут баллы по одному предмету, а по оси y — по второму:\n",
    "\n",
    "img\n",
    "Источник изображения\n",
    "Далее мы вводим две новые оси, которые являются линейными комбинациями предыдущих. Ось PC1 проходит через максимальную дисперсию данных и является суммой исходных осей, а ось PC2 является разницей двух исходных осей, перпендикулярна первой оси и тоже проходит через максимальную оставшуюся дисперсию:\n",
    "\n",
    "img\n",
    "Таким образом, мы получили первую главную компоненту и вторую компоненту. При этом первая компонента несёт в себе максимум информации о данных, а в каждой последующей компоненте информации всё меньше.\n",
    "\n",
    "Рассмотрим, как запустить PCA с помощью библиотеки sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# из модуля decomposition библиотеки sklearn импортируем класс PCA\n",
    "from sklearn.decomposition import PCA\n",
    "# создаём объект класса PCA\n",
    "# n_components — задаём количество компонентов для проведения трансформации\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "# обучаем модель на данных X\n",
    "pca.fit(X)\n",
    "# применяем уменьшение размерности к матрице X\n",
    "pca.transform(X)\n",
    "# или сразу\n",
    "pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим работу с PCA на практике. Из датасетов sklearn импортируем датасет MNIST — это данные, основанные на рукописном начертании цифр:\n",
    "\n",
    "img\n",
    "Источник изображения\n",
    "Для данного датасета обучим модель, которая по пикселям изображения предсказывает, что за цифра на нём изображена. Первой мы разработаем модель, которая будет учитывать все признаки, а затем уменьшим размерность данных с помощью PCA и ещё раз обучим модель. Далее мы сравним качество полученных моделей и время, которое было затрачено на обучение в каждом случае.\n",
    "\n",
    "В датасете MNIST представлено 70 000 изображений, каждое из которых описывается 784 признаками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим датасет MNIST\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = fetch_openml(\"mnist_784\")\n",
    "# загрузим признаки в переменную X  \n",
    "X = dataset['data']\n",
    "# загрузим «ответы» в переменную y\n",
    "y = dataset['target']\n",
    "# разделим данные с помощью sklearn на данные для обучения и теста\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "# импортируем StandardScaler для стандартизации данных\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# создадим объект класса StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x)\n",
    "# трансформируем датасеты train_x и test_x\n",
    "train_x = scaler.transform(train_x)\n",
    "test_x = scaler.transform(test_x)\n",
    "\n",
    "# импортируем класс PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# создадим объект класса PCA\n",
    "pca = PCA(n_components=300)\n",
    "pca.fit(train_x)\n",
    "# уменьшим размерность данных\n",
    "train_x_pca = pca.transform(train_x)\n",
    "test_x_pca = pca.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, сколько признаков описывало объект до и после уменьшения размерности. Как мы можем заметить, сначала было 787 признаков, а в конце объект описывают уже 300 главных компонент:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x[0]))\n",
    "print(len(train_x_pca[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель логистической регрессии, которая на вход будет принимать пиксели изображения и предсказывать, что на нём нарисовано.\n",
    "\n",
    "Напишем функцию, которая будет принимать на вход данные для обучения (матрицу с признаками и правильные ответы) и данные для тестирования модели, а на выходе будет возвращать время, затраченное на обучение модели, и качество модели. В качестве метрики оценивания качества будем использовать метрику accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель, построенная на признаках, полученных после уменьшения размерности. Время обучения 29.03356122970581, метрика модели 0.9258571428571428\n",
      "Модель, построенная на всех исходных признаках. Время обучения 40.17377543449402, метрика модели 0.9188571428571428\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "\n",
    "# напишем функцию, которая на вход принимает X и y, а возвращает модель и время\n",
    "def get_time_and_accuracy(train_x, train_y, test_x, test_y):\n",
    "    # создадим объект класса LogisticRegression\n",
    "    log_reg_model = LogisticRegression(max_iter=1000)\n",
    "    # запишем время с начала эпохи в секундах до обучения модели\n",
    "    start_time = time()\n",
    "    # обучим модель\n",
    "    log_reg_model.fit(train_x, train_y)    \n",
    "    # запишем время с начала эпохи в секундах после обучения\n",
    "    end_time = time()\n",
    "    # подсчитаем время, потраченное на обучение модели\n",
    "    delta_time = end_time-start_time\n",
    "    # предскажем на тестовых данных\n",
    "    y_pred = log_reg_model.predict(test_x)\n",
    "    # посчитаем скор для тестового предсказания\n",
    "    score = accuracy_score(test_y, y_pred)\n",
    "    # вернём время, потраченное на обучение, и качество полученной модели\n",
    "    return delta_time, score\n",
    "\n",
    "model_pca_time, model_pca_acc = get_time_and_accuracy(train_x_pca, train_y, test_x_pca, test_y)\n",
    "print(f\"Модель, построенная на признаках, полученных после уменьшения размерности. Время обучения {model_pca_time}, метрика модели {model_pca_acc}\")\n",
    "# Модель, построенная на признаках, полученных после уменьшения размерности. Время обучения 54.12072825431824, метрика модели 0.9255714285714286\n",
    "\n",
    "model_time, model_acc = get_time_and_accuracy(train_x, train_y, test_x, test_y)\n",
    "print(f\"Модель, построенная на всех исходных признаках. Время обучения {model_time}, метрика модели {model_acc}\")\n",
    "# Модель, построенная на всех исходных признаках. Время обучения 108.04033303260803, метрика модели 0.9187142857142857"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Время на обучение\tКачество модели\n",
    "Признаки после PCA\t54.12\t0.9256\n",
    "Исходные признаки\t108.04\t0.9187\n",
    "Как видим, в данном случае мы потратили на обучение модели в два раза меньше времени, а качество осталось практически таким же.\n",
    "\n",
    "В реальной работе бывает гораздо больше данных и на обучение модели уходит отнюдь не две минуты. Таким образом, применив технику уменьшения размерности, можно существенно сэкономить время.\n",
    "\n",
    "→ Мы научились ускорять обучение моделей с помощью понижения размерности данных, а теперь давайте научимся визуализировать многомерное пространство."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE\n",
    "\n",
    "Для того чтобы визуализировать многомерное пространство признаков, необходимо уменьшить его размерность до двухмерного или трёхмерного. В этом поможет t-SNE (t-distributed Stochastic Neighbor Embedding), что переводится с английского как «стохастическое вложение соседей с t-распределением». \n",
    "\n",
    "t-SNE, в отличие от PCA, использует технику нелинейного снижения размерности данных. Обычно используется снижение размерности многомерного пространства до двух- или трёхмерного с целью дальнейшей визуализации. При преобразовании похожие объекты оказываются рядом, а непохожие — далеко друг от друга.\n",
    "\n",
    "Рассмотрим t-SNE на примере понижения размерности двухмерного пространства до одномерного.\n",
    "\n",
    "Если у нас есть такое распределение точек, как на графике ниже,\n",
    "\n",
    "img\n",
    "то, казалось бы, чтобы уменьшить размерность данных, нужно просто спроецировать эти точки на ось x:\n",
    "\n",
    "img\n",
    "Но, как мы видим, розовый и синий кластеры перестают быть различными кластерами, когда мы проецируем данные на ось X — данные перемешались. t-SNE позволяет не допускать такого.\n",
    "\n",
    "Алгоритм состоит из следующих шагов:\n",
    "\n",
    "1\n",
    "В исходном многомерном пространстве для каждого объекта из датасета рассчитываем евклидово расстояние между объектами. Далее с помощью нормального распределения попарно сравниваем объекты датасета. На основе этих значений строится матрица, которая содержит значения схожести объектов:\n",
    "\n",
    "img\n",
    "На рисунке выше показано, как для одной точки рассчитали расстояние до всех других точек датасета, далее получили схожесть объектов с помощью нормального распределения и построили матрицу. На данной матрице по строкам и столбцам находятся объекты датасета, а в ячейках — значения схожести двух объектов. Красным обозначена похожесть объекта на самого себя (в таком случае это значение максимально), а розовым — объекты, которые имеют большую схожесть.\n",
    "\n",
    "2\n",
    "На втором шаге мы уменьшаем размерность данных. Здесь мы случайным образом проецируем объекты из двухмерного пространства на ось x. Далее, как и на первом шаге, мы считаем схожесть объектов в новом пространстве, но для подсчёта используем не нормальное распределение, а t-распределение. После этого строим матрицу попарной схожести объектов:\n",
    "\n",
    "img\n",
    "3\n",
    "Далее необходимо создать новую матрицу сходства, которая будет похожа на исходную:\n",
    "\n",
    "img\n",
    "С каждой итерацией точки перемещаются к своим ближайшим соседям из исходного многомерного пространства и удаляются от отдалённых:\n",
    "\n",
    "img\n",
    "Таким образом, мы итеративно приходим к разделению объектов в новом пространстве.\n",
    "\n",
    "Работа алгоритма выглядит так:\n",
    "\n",
    "img\n",
    "На первой итерации данные расположены хаотично, но с каждой итерацией похожие объекты подходят ближе друг к другу, а непохожие отдаляются друг от друга.\n",
    "\n",
    "Как запустить t-SNE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем класс TSNE из модуля manifold библиотеки sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# создаём объект класса TSNE\n",
    "# n_components — размерность нового пространства\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=500, random_state=42)\n",
    "# обучаем модель на данных X и производим трансформацию\n",
    "tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важные параметры для запуска:\n",
    "\n",
    "n_components — размерность нового пространства.\n",
    "perplexity — один из важнейших параметров для запуска. Этот параметр описывает ожидаемую плотность вокруг точки. Таким образом мы можем устанавливать соотношение ближайших соседей к точке. Если датасет большой, стоит установить большее значение perplexity. Обычно используют значения в диапазоне от 5 до 50.\n",
    "n_iter — количество итераций для оптимизации.\n",
    "random_state — так как в алгоритме есть случайность, задание random_state позволяет от запуска к запуску получать одинаковые результаты.\n",
    "Теперь попробуем уменьшить размерность и визуализировать пространство пикселей, которые описывают данные рукописного начертания цифр. Сравним, какая визуализация получается при использовании PCA и tSNE.\n",
    "\n",
    "Уменьшим размерность с помощью PCA:\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# создадим объект класса PCA, уменьшим размерность данных до 2\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "# уменьшим размерность данных\n",
    "X_reduced = pca.fit_transform(train_x)\n",
    "# сохраним данные в датафрейм\n",
    "df_pca = pd.DataFrame(X_reduced)\n",
    "# сохраним разметки кластеров\n",
    "df_pca['c'] = pd.to_numeric(train_y).astype('Int64').to_list()\n",
    "# визуализируем\n",
    "sns.scatterplot(x=df_pca[0], y=df_pca[1], c=df_pca['c'])\n",
    "img\n",
    "Уменьшим размерность с помощью t-SNE:\n",
    "\n",
    "# создадим объект класса TSNE, уменьшим размерность данных до 2\n",
    "tsne = TSNE(n_components=2, perplexity=50, n_iter=500, random_state=42)\n",
    "# уменьшим размерность данных\n",
    "X_reduced = tsne.fit_transform(train_x)\n",
    "# сохраним данные в датафрейм\n",
    "df_tsne = pd.DataFrame(X_reduced)\n",
    "# сохраним разметки кластеров\n",
    "df_tsne['c'] = pd.to_numeric(train_y).astype('Int64').to_list()# визуализируем\n",
    "sns.scatterplot(x=df_tsne[0], y=df_tsne[1], c=df_tsne['c'])\n",
    "img\n",
    "Как видим, алгоритм t-SNE отлично справляется с уменьшением размерности для визуализации, а вот при использовании PCA данные не разделились на кластеры и пересекаются друг с другом — такую визуализацию будет неудобно анализировать.\n",
    "\n",
    "В данном юните мы изучили несколько техник уменьшения размерности данных. Эти методы используются для разных целей. Давайте кратко подведём итоги и составим сводную таблицу:\n",
    "\n",
    "PCA\tt-SNE\n",
    "Преобразование, которое используется для снижения размерности\tЛинейное\tНелинейное\n",
    "Кейсы использования\tУменьшение размерности данных с целью их дальнейшего использования в ML-моделях\tВизуализация многомерного пространства\n",
    "Вызов в sklearn\t\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
