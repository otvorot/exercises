{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ Ранее мы обсуждали модель линейной регрессии, которая предназначена для решения задачи регрессии. Теперь нам предстоит разобраться с тем, как преобразовать данную модель, чтобы она решала задачу классификации.\n",
    "\n",
    "Для начала вспомним, что такое классификация.\n",
    "\n",
    "Задача классификации (classification) — задача, в которой мы пытаемся предсказать класс объекта на основе признаков в наборе данных. То есть задача сводится к предсказанию целевого признака, который является категориальным.\n",
    "\n",
    "Когда классов, которые мы хотим предсказать, только два, классификация называется бинарной. Например, мы можем предсказать, болен ли пациент раком, является ли изображение человеческим лицом, является ли письмо спамом и т. д.\n",
    "\n",
    "Когда классов, которые мы хотим предсказать, более двух, классификация называется мультиклассовой (многоклассовой). Например, предсказание модели самолёта по радиолокационным снимкам, классификация животных на фотографиях, определение языка, на котором говорит пользователь, разделение писем на группы.\n",
    "\n",
    "img\n",
    "\n",
    "→ Для простоты мы пока разберёмся с бинарной классификацией, а в следующем юните обобщим результат на мультиклассовую.\n",
    "\n",
    "Что вообще означает «решить задачу классификации»? Это значит построить разделяющую поверхность в пространстве признаков, которая делит пространство на части, каждая из которых соответствует определённому классу. \n",
    "\n",
    "Ниже представлены примеры разделяющих поверхностей, которые производят бинарную классификацию. Красным и синим цветом обозначены классы, зелёным — собственно поверхность, которая делит пространство признаков на две части. В каждой из этих частей находятся только наблюдения определённого класса.\n",
    "\n",
    "\n",
    "Модели, которые решают задачу классификации, называются классификаторами (classifier).\n",
    "\n",
    "Если взять в качестве разделяющей поверхности некоторую плоскость (ровная поверхность на первом рисунке), то мы получаем модель логистической регрессии, которая тесно связана с рассмотренной нами ранее линейной регрессией.\n",
    "\n",
    "В общем случае это уравнение гиперплоскости, которая стремится приблизить зависимость целевой переменной от  факторов.\n",
    "\n",
    "Когда фактор всего один, уравнение задаёт прямую:\n",
    "Когда факторов два, уравнение задаёт плоскость:\n",
    "→ Но всё это работает только в том случае, когда целевой признак , который мы хотим предсказать, является числовым, например цена, вес, время аренды и т. д.\n",
    "\n",
    "Что же делать с этой моделью, когда целевой признак  является категориальным? Например, является письмо спамом или обычным письмом?\n",
    "\n",
    "Можно предположить, что, раз у нас есть две категории, мы можем обозначить категории за  (Спам) и  (Не спам) и обучить линейную регрессию предсказывать 0 и 1.\n",
    "\n",
    "Но результат будет очень плохим. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для больших значений  прямая будет выдавать значения больше 1, а для очень маленьких — меньше 0. Что это значит? Непонятно. Непонятно и то, что делать со значениями в диапазоне от 0 до 1. Да, можно относить значения на прямой выше 0.5 к классу 1, а меньше либо равным 0.5 — к классу 0, но это всё «костыли».\n",
    "\n",
    "Идея! Давайте переведём задачу классификации в задачу регрессии. Вместо предсказания класса будем предсказывать вероятность принадлежности к этому классу. \n",
    "\n",
    "Модель должна выдавать некоторую вероятность P, которая будет определять, принадлежит ли данный объект к классу 1: например, вероятность того, что письмо является спамом. При этом вероятность того, что письмо является обычным письмом (класс 0), определяется как Q = 1 - P.  \n",
    "\n",
    "Когда модель будет обучена на предсказание вероятности, мы зададим некоторый порог вероятности. Если предсказанная вероятность будет выше этого порога, мы определим объект к классу 1, а если ниже — к классу 0.\n",
    "\n",
    "\n",
    "Например, стандартный порог равен 0.5. То есть если вероятность P > 0.5, мы будем считать письмо спамом, а если <= 0.5 — обычным информативным письмом.\n",
    "\n",
    "В итоге мы добьёмся того, что будем предсказывать не дискретный категориальный, а непрерывный числовой признак, который лежит в диапазоне [0, 1]. А это уже знакомая нам задача регрессии.\n",
    "\n",
    "→ Однако остался главный вопрос: как научить модель предсказывать вероятности, ведь они должны лежать строго в диапазоне от 0 до 1, а предсказания линейной регрессии лежат в диапазоне от и до +- бесконечноси? \n",
    "\n",
    "Тут-то мы и приходим к модели логистической регрессии — регрессии вероятностей.\n",
    "\n",
    "\n",
    "ОБЩЕЕ ПРЕДСТАВЛЕНИЕ О ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ\n",
    "\n",
    "Логистическая регрессия (Logistic Regression) — одна из простейших моделей для решения задачи классификации. Несмотря на простоту, модель входит в топ часто используемых алгоритмов классификации в Data Science.\n",
    "\n",
    "В основе логистической регрессии лежит логистическая функция (logistic function)  — отсюда и название модели. Однако более распространённое название этой функции — сигмόида (sigmoid).\n",
    "\n",
    "\n",
    "Выходом сигмоиды является число от 0 до 1, которое можно интерпретировать как вероятность принадлежности к классу 1. Её мы и пытаемся предсказать.\n",
    "\n",
    "Основная идея модели логистической регрессии: возьмём модель линейной регрессии (обозначим её выход за Z)\n",
    "и подставим выход модели Z в функцию сигмоиды, чтобы получить искомые оценки вероятности (в математике принято писать оценочные величины с «шапкой» наверху, а истинные значения — без «шапки», это чистая формальность)\n",
    "\n",
    "Чего мы добились таким преобразованием?\n",
    "\n",
    "Если мы обучим модель, то есть подберём коэффициенты w0, w1, w2, w3,..., wm (как их найти, обсудим чуть позже) таким образом, что для объектов класса 1 модель линейной регрессии начнёт выдавать положительное число, а для класса 0 — выдавать отрицательное число, то тогда, подставив предсказание линейной регрессии Z в сигмоиду, мы сможем получать вероятности принадлежности к каждому из классов в диапазоне от 0 до 1.\n",
    "\n",
    "Далее по порогу вероятности мы сможем определять, к какому классу принадлежит объект.\n",
    "\n",
    "Это и есть наша цель. Мы свели задачу классификации к задаче регрессии для предсказания вероятностей. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ПОИСК ПАРАМЕТРОВ ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ\n",
    "\n",
    "Итак, мы разобрались с тем, как выглядит модель логистической регрессии и что она означает в геометрическом смысле.\n",
    "\n",
    "Но остался один главный вопрос: как найти такие коэффициенты , чтобы гиперплоскость разделяла пространство наилучшим образом?\n",
    "\n",
    "Можно предположить, что стоит использовать метод наименьших квадратов. Введём функцию ошибки — средний квадрат разности MSE между истинными классами  и предсказанными классами  и попытаемся его минимизировать.\n",
    "\n",
    "Сразу можно достоверно предсказать, что результат такого решения будет плохим, поэтому воздержимся от его использования.\n",
    "\n",
    "Здесь нужен другой подход. Это метод максимального правдоподобия (Maximum Likelihood Estimation — MLE). \n",
    "\n",
    "Правдоподобие — это оценка того, насколько вероятно получить истинное значение целевой переменной y при данных x и параметрах w.\n",
    "\n",
    "Примечание. К сожалению, функция likelihood не имеет интерпретации, то есть нельзя сказать, что значит число -2.34 в контексте правдоподобия.\n",
    "\n",
    "Цель — найти такие параметры, при которых наблюдается максимум этой функции.\n",
    "\n",
    "Теперь пора снова применить магию математики, чтобы привести задачу к привычному нам формату минимизации эмпирического риска. По правилам оптимизации, если поставить перед функцией минус, то задача оптимизации меняется на противоположную: был поиск максимума — станет поиском минимума.\n",
    "\n",
    "Таким образом мы получим функцию потерь , которая носит название «функция логистических потерь», или logloss. Также часто можно встретить название кросс-энтропия, или cross-entropy loss:\n",
    "\n",
    "Вот эту функцию мы и будем минимизировать в рамках поиска параметров логистической регрессии. Мы должны найти такие параметры разделяющей плоскости  , при которых наблюдается минимум logloss.\n",
    "\n",
    "Знакомая задача? Всё то же самое, что и с линейной регрессией, только функция ошибки другая.\n",
    "\n",
    "→ К сожалению, для такой функции потерь аналитическое решение оптимизационной задачи найти не получится: при расчётах получается, что его попросту не существует.\n",
    "\n",
    "Но мы помним, что, помимо аналитических решений, есть и численные.\n",
    "\n",
    "Например, для поиска параметров можно использовать знакомый нам градиентный спуск. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Повторим её смысл: новое значение параметров  получается путём сдвига текущих  в сторону вектора антиградиента , умноженного на темп обучения .\n",
    "\n",
    "Математическую реализацию вычисления градиента для logloss мы обсудим далее в курсе, а пока нас интересует исключительно его смысл.\n",
    "\n",
    "Формула градиентного спуска для logloss (для любознательных)\n",
    "Мы уже знаем, что для того, чтобы повысить шанс пройти мимо локальных минимумов функции потерь, используется не сам градиентный спуск, а его модификации: например, можно использовать уже знакомый нам стохастический градиентный спуск (SGD).\n",
    "\n",
    "Помним, что применение градиентного спуска требует предварительного масштабирования данных (стандартизации/нормализации). В реализации логистической регрессии в sklearn предусмотрено ещё несколько методов оптимизации, для которых масштабирование не обязательно. О них мы упомянем в практической части модуля.\n",
    "\n",
    "Во избежание переобучения модели в функцию потерь логистической регрессии традиционно добавляется регуляризация. В реализации логистической регрессии в sklearn она немного отличается от той, что мы видели ранее для линейной регрессии.\n",
    "\n",
    "При L1-регуляризации мы добавляем в функцию потерь  штраф из суммы модулей параметров, а саму функцию logloss умножаем на коэффициент :\n",
    "\n",
    "А при L2-регуляризации — штраф из суммы квадратов параметров:\n",
    "\n",
    "Значение коэффициента  — коэффициент, обратный коэффициенту регуляризации. Чем больше , тем меньше «сила» регуляризации.\n",
    "\n",
    "Примечание для любознательных\n",
    "Предлагаем вам посмотреть на то, как будет меняться форма сигмоиды, разделяющей плоскости при минимизации функции потерь logloss (она обозначена как cross-entropy в виде концентрических кругов — вид сверху), с помощью обычного градиентного спуска (не стохастического) в виде анимации.\n",
    "\n",
    "img\n",
    "\n",
    "img\n",
    "\n",
    "img\n",
    "\n",
    "Источник изображений\n",
    "\n",
    "Не волнуйтесь, все громоздкие формулы уже реализованы в классических библиотеках, таких как sklearn. Но нам важно понимать принцип того, что происходит «под капотом», чтобы верно интерпретировать результат и по возможности управлять им.\n",
    "\n",
    "Теперь давайте перейдём к практической реализации логистической регрессии и решим с её помощью задачу классификации. Но предварительно закрепим теоретический материал ↓\n",
    "\n",
    "Задание 2.6\n",
    "1 point possible (graded)\n",
    "Из какого метода выводится выражение для функции потерь в логистической регрессии?\n",
    "Метод наименьших квадратов\n",
    "Уравнение Беллмана\n",
    "Метод минимального правдоподобия\n",
    "Метод максимального правдоподобия\n",
    "нет ответа\n",
    "Отправить\n",
    "Some problems have options such as save, reset, hints, or show answer. These options follow the Submit button.\n",
    "Задание 2.7\n",
    "1 point possible (graded)\n",
    "Как называется функция потерь, которая минимизируется при обучении логистической регрессии?\n",
    "Средний квадрат ошибки (Mean Squared Error, MSE)\n",
    "Логистическая функция потерь (logistic loss) или кросс-энтропия (cross-entropy)\n",
    "Средняя абсолютная ошибка (Mean Absolute Error, MAE)\n",
    "Достоверность (Accuracy)\n",
    "нет ответа\n",
    "Отправить\n",
    "Some problems have options such as save, reset, hints, or show answer. These options follow the Submit button.\n",
    "ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ В SKLEARN\n",
    "\n",
    "→ Скачайте ноутбук с кодом\n",
    "\n",
    "Мы будем работать со знакомым вам по модулю «Очистка данных» набором данных о диабете, первоначально полученным в Национальном институте диабета, болезней органов пищеварения и почек.\n",
    "\n",
    "Наша цель будет состоять в том, чтобы диагностически предсказать, есть ли у пациента диабет. На выбор экземпляров из более крупной базы данных было наложено несколько ограничений. В частности, все пациенты здесь — женщины не моложе 21 года из индейского племени Пима.\n",
    "\n",
    "В модуле по очистке данных вы уже производили очистку этого набора данных:\n",
    "\n",
    "удалили дубликаты,\n",
    "удалили неинформативный признак Gender,\n",
    "обработали «скрытые» пропуски в данных,\n",
    "избавились от потенциальных выбросов.\n",
    "Если забыли, загляните в соответствующий раздел модуля.\n",
    "\n",
    "Очищенный датасет вы можете найти здесь.\n",
    "\n",
    "Итак, импортируем библиотеки, необходимые нам для работы с данными:\n",
    "\n",
    "import numpy as np #для матричных вычислений\n",
    "import pandas as pd #для анализа и предобработки данных\n",
    "import matplotlib.pyplot as plt #для визуализации\n",
    "import seaborn as sns #для визуализации\n",
    "\n",
    "\n",
    "import warnings # для игнорирования предупреждений\n",
    "#Игнорируем предупреждения\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Устанавливаем стиль визуализаций в matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "Затем прочитаем нашу таблицу:\n",
    "\n",
    "diabetes_data = pd.read_csv('data/diabetes_cleaned.csv')\n",
    "diabetes_data.head()\n",
    "img\n",
    "\n",
    "Напомним, какие признаки у нас есть:\n",
    "\n",
    "Pregnancies — количество беременностей;\n",
    "Glucose — концентрация глюкозы в плазме через два часа при пероральном тесте на толерантность к глюкозе;\n",
    "BloodPressure — диастолическое артериальное давление (мм рт. ст.);\n",
    "SkinThickness — толщина кожной складки трицепса (мм);\n",
    "BMI — индекс массы тела (\n",
    "в\n",
    "е\n",
    "с\n",
    "в\n",
    "к\n",
    "г\n",
    "р\n",
    "о\n",
    "с\n",
    "т\n",
    "в\n",
    "м\n",
    ");\n",
    "DiabetesPedigreeFunction — функция родословной диабета (чем она выше, тем выше шанс наследственной заболеваемости);\n",
    "Age — возраст;\n",
    "Outcome — наличие диабета (0 — нет, 1 — да), целевой признак.\n",
    "Размер таблицы:\n",
    "\n",
    "print(diabetes_data.shape)\n",
    "#(757, 8)\n",
    "Давайте посмотрим на матрицу корреляций и выберем наиболее коррелированные с целевым признаком факторы:\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8, 4))\n",
    "sns.heatmap(diabetes_data.corr(), annot=True);\n",
    "img\n",
    "\n",
    "Из строки Outcome видно, что наибольшей корреляцией с целевым признаком обладают факторы Glucose (уровень глюкозы) и BMI (индекс массы тела).\n",
    "\n",
    "Построим диаграмму рассеяния, по оси абсцисс отложим фактор Glucose, а по оси ординат — BMI. Сделаем цветовую группировку по признаку наличия диабета:\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 5)) #фигура + координатная плоскость\n",
    "#Строим диаграмму рассеяния\n",
    "sns.scatterplot(\n",
    "    data=diabetes_data, #датасет\n",
    "    x='Glucose', #ось абсцисс\n",
    "    y='BMI', #ось ординат\n",
    "    hue='Outcome', #группировка\n",
    "    palette='seismic', #палитра\n",
    "    ax=ax, #координатная плоскость\n",
    ");\n",
    "\n",
    "#Задаём название графику\n",
    "ax.set_title('Scatter Plot for Glucose VS BMI');\n",
    "img\n",
    "\n",
    "Видно, что классы довольно сильно перемешаны, но красные точки, соответствующие пациентам с диабетом, преимущественно сгруппированы в правой части системы координат.\n",
    "\n",
    "Давайте попробуем построить разделяющую плоскость с помощью логистической регрессии.\n",
    "\n",
    "Для начала разделим выборку на матрицу наблюдений X, состоящую из двух признаков, и столбец с правильным ответами y:\n",
    "\n",
    "#Создаём матрицу наблюдений X и столбец с ответами y\n",
    "X = diabetes_data[['Glucose', 'BMI']]\n",
    "y = diabetes_data['Outcome']\n",
    "Логистическая регрессия — линейная модель, поэтому она находится в уже знакомом нам модуле linear_model из библиотеки sklearn.\n",
    "\n",
    "from sklearn import linear_model #линейные модели\n",
    "В модуле находится класс LogisticRegression, который реализует поиск коэффициентов разделяющей плоскости путём минимизации функции потерь logloss различными численными методами.\n",
    "\n",
    "Кликните на плашку, чтобы увидеть информацию ↓\n",
    "\n",
    "Основные параметры LogisticRegression\n",
    "\n",
    "Обучим логистическую регрессию, сделав значением параметра random_state, например, число 42 (традиционное число, которое означает ответ на вопрос о смысле жизни), чтобы гарантировано получить одинаковые результаты. Остальные параметры оставим по умолчанию.\n",
    "\n",
    "Для обучения модели необходимо просто вызвать метод fit(), передав в него матрицу наблюдений X и вектор правильных ответов y.\n",
    "\n",
    "Чтобы получить параметр , нужно обратиться к атрибуту intercept_, а вектор параметров  , , ...,  будет храниться в атрибуте coef_ (так как в матрице X два фактора, то и коэффициента будет два):\n",
    "\n",
    "#Создаём объект класса LogisticRegression\n",
    "log_reg_2d = linear_model.LogisticRegression(random_state=42)\n",
    "#Обучаем модель, минимизируя logloss\n",
    "log_reg_2d.fit(X, y)\n",
    "#Выводим результирующие коэффициенты\n",
    "print('w0: {}'.format(log_reg_2d.intercept_)) #свободный член w0\n",
    "print('w1, w2: {}'.format(log_reg_2d.coef_)) #остальные параметры модели w1, w2, ..., wm\n",
    "\n",
    "# w0: [-8.24898965]\n",
    "# w1, w2: [[0.03779275 0.0875742 ]]\n",
    "→ В отличие от линейной регрессии, коэффициенты логистической регрессии интерпретировать сложнее. Мы не будем этим заниматься.\n",
    "\n",
    "Как нам сделать предсказание вероятности наличия диабета у пациента?\n",
    "\n",
    "Мы должны подставить значения факторов в уравнение разделяющей плоскости, а затем результат подставить в функцию сигмоиды. Как хорошо, что для этого есть метод predict_proba().\n",
    "\n",
    "Например, появился новый пациент с концентрацией глюкозы 180 мг/100мл и индексом массы тела в \n",
    "к\n",
    "г\n",
    "м\n",
    ". Мы хотим вычислить вероятность наличия/отсутствия диабета у данного пациента:\n",
    "\n",
    "#Значения концентрации глюкозы и индекса массы тела для пациента\n",
    "x_new = pd.DataFrame({'Glucose': [180], 'BMI': [51]})\n",
    "#Делаем предсказание вероятностей:\n",
    "y_new_proba_predict = log_reg_2d.predict_proba(x_new)\n",
    "print('Predicted probabilities: {}'.format(np.round(y_new_proba_predict, 2)))\n",
    "\n",
    "# Predicted probabilities: [[0.05 0.95]]\n",
    "Метод predict_proba() возвращает сразу две вероятности: первая соответствует вероятности принадлежности к классу 0 (диабета нет), а вторая — вероятности принадлежности к классу 1 (диабет есть). Заметьте, что в сумме две вероятности дают 1, что вполне логично, так как события взаимоисключающие.\n",
    "\n",
    "Если мы хотим предсказать не вероятности, а сам класс (1 или 0), нам пригодится метод predict(). По умолчанию метод predict() относит объект к классу 1, если вероятность принадлежности к классу 1 > 0.5, и к классу 0, если эта вероятность < 0.5.\n",
    "\n",
    "#Значения концентрации глюкозы и индекса массы тела для пациента\n",
    "x_new = pd.DataFrame({'Glucose': [180], 'BMI': [51]})\n",
    "#Делаем предсказание класса:\n",
    "y_new_predict = log_reg_2d.predict(x_new)\n",
    "print('Predicted class: {}'.format(y_new_predict))\n",
    "# Predicted class: [1]\n",
    "Предсказанный класс равен 1: значит, модель считает, что данный пациент болеет диабетом.\n",
    "\n",
    "Теперь построим визуализацию нашей модели.\n",
    "\n",
    "Чтобы красиво визуализировать вероятности в виде тепловой карты, мы подготовили для вас специальную функцию — plot_probabilities_2d().\n",
    "\n",
    "Данная функция принимает три аргумента:\n",
    "\n",
    "X — матрица с наблюдениями,\n",
    "y — столбец с правильными ответами,\n",
    "model — модель, с помощью которой делается предсказание вероятностей.\n",
    "→ Мы не будем подробно останавливаться на работе данной функции — она нам нужна только для красивой визуализации, но если вы захотите разобраться, мы написали подробные комментарии к каждой строчке кода.\n",
    "\n",
    "#Функция для визуализации модели\n",
    "def plot_probabilities_2d(X, y, model):\n",
    "    #Генерируем координатную сетку из всех возможных значений для признаков\n",
    "    #Glucose изменяется от x1_min = 44 до x2_max = 199, \n",
    "    #BMI — от x2_min = 18.2 до x2_max = 67.1\n",
    "    #Результат работы функции — два массива xx1 и xx2, которые образуют координатную сетку\n",
    "    xx1, xx2 = np.meshgrid(\n",
    "        np.arange(X.iloc[:, 0].min()-1, X.iloc[:, 0].max()+1, 0.1),\n",
    "        np.arange(X.iloc[:, 1].min()-1, X.iloc[:, 1].max()+1, 0.1)\n",
    "    )\n",
    "    #Вытягиваем каждый из массивов в вектор-столбец — reshape(-1, 1)\n",
    "    #Объединяем два столбца в таблицу с помощью hstack\n",
    "    X_net = np.hstack([xx1.reshape(-1, 1), xx2.reshape(-1, 1)])\n",
    "    #Предсказываем вероятность для всех точек на координатной сетке\n",
    "    #Нам нужна только вероятность класса 1\n",
    "    probs = model.predict_proba(X_net)[:, 1]\n",
    "    #Переводим столбец из вероятностей в размер координатной сетки\n",
    "    probs = probs.reshape(xx1.shape)\n",
    "    #Создаём фигуру и координатную плоскость\n",
    "    fig, ax = plt.subplots(figsize = (10, 5))\n",
    "    #Рисуем тепловую карту вероятностей\n",
    "    contour = ax.contourf(xx1, xx2, probs, 100, cmap='bwr')\n",
    "    #Рисуем разделяющую плоскость — линию, где вероятность равна 0.5\n",
    "    bound = ax.contour(xx1, xx2, probs, [0.5], linewidths=2, colors='black');\n",
    "    #Добавляем цветовую панель \n",
    "    colorbar = fig.colorbar(contour)\n",
    "    #Накладываем поверх тепловой карты диаграмму рассеяния\n",
    "    sns.scatterplot(x=X.iloc[:, 0], y=X.iloc[:, 1], hue=y, palette='seismic', ax=ax)\n",
    "    #Даём графику название\n",
    "    ax.set_title('Scatter Plot with Decision Boundary');\n",
    "    #Смещаем легенду в верхний левый угол вне графика\n",
    "    ax.legend(bbox_to_anchor=(-0.05, 1))\n",
    "Теперь вызовем нашу функцию и посмотрим, как логистическая вероятность делит пространство на две части.\n",
    "\n",
    "plot_probabilities_2d(X, y, log_reg_2d)\n",
    "img\n",
    "\n",
    "Мы видим разделяющую поверхность (обозначена чёрной линией). Она делит пространство на две части: красное соответствует точкам, которые будут отнесены моделью к классу 1, а синее соответствует точкам, которые будут отнесены моделью к классу 0.\n",
    "\n",
    "Справа от графика есть цветовая панель, которая показывает значение вероятности того, что пациент болен диабетом (по мнению модели).\n",
    "\n",
    "Обратите внимание, что чем более точка удалена от разделяющей плоскости, тем с большей вероятностью модель относит её к тому или иному классу. На тепловой карте это прослеживается в интенсивности синего и красного цвета.\n",
    "\n",
    "Из графика видно, что довольно много синих точек попали в красную зону и наоборот. Видимо, информации о двух факторах всё же маловато для того, чтобы хорошо провести классификацию.\n",
    "\n",
    "Давайте будем использовать для построения модели всю предоставленную нам информацию — все факторы из набора данных. Для этого заново создадим матрицу наблюдений X и вектор ответов y:\n",
    "\n",
    "#Создаём матрицу признаков X и столбец с ответами y\n",
    "X = diabetes_data.drop('Outcome', axis=1)\n",
    "y = diabetes_data['Outcome']\n",
    "Затем обучим модель логистической регрессии с помощью метода fit().\n",
    "\n",
    "Сделаем предсказание вероятности заболевания диабетом для каждого пациента и самих классов с помощью методов predict_proba() и predict().\n",
    "\n",
    "#Создаём объект класса LogisticRegression\n",
    "log_reg_full = linear_model.LogisticRegression(random_state=42, max_iter=1000)\n",
    "#Обучаем модель, минимизируя logloss\n",
    "log_reg_full.fit(X, y)\n",
    "#Делаем предсказание класса\n",
    "y_pred = log_reg_full.predict(X)\n",
    "Затем добавим предсказание в таблицу X для наглядности:\n",
    "\n",
    "#Создадим временную таблицу X\n",
    "X_temp = X.copy()\n",
    "#Добавим в эту таблицу результат предсказания\n",
    "X_temp['Prediction'] = y_pred\n",
    "X_temp.tail()\n",
    "img\n",
    "\n",
    "Итак, мы сделали предсказание для каждого наблюдения (пациента) из таблицы X.\n",
    "\n",
    "К сожалению, теперь, когда признаков больше двух, построить красивую визуализацию разделяющей плоскости не получится.\n",
    "\n",
    "→ Но как же тогда узнать, насколько хорошо модель определяет наличие диабета у пациента? Для этого используются метрики классификации, о которых мы поговорим в следующем юните."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
